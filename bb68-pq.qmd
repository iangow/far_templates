---
title: "Exercise template for 'Ball and Brown (1968)'"
author: Your name
format: html
bibliography: book.bib
---

## Important instructions

```{r}
#| message: false
#| include: false
library(dplyr, warn.conflicts = FALSE)
library(DBI)
library(readr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(dbplyr)     # For window_order()
library(farr)
```

```{r}
#| include: false
#| eval: false
# If necessary, change the line below to reflect the location of 
# the Parquet file repository on your computer and change "eval: false" above
# to "eval: true".
Sys.setenv(DATA_DIR = "~/Dropbox/pq_data")
```

# @Ball:1968ub {#bb1968}

## Principal results of @Ball:1968ub

### Discussion questions

1.	What is the research question of @Ball:1968ub? 
Do you find the paper to provide a persuasive answer to its research question?

2.	Look at the references in @Ball:2019wu [pp. 177-178].
What do you notice?

3. Given that "the most fundamental result" of @Ball:1968ub relates to an association or correlation, is it correct to say that the paper provides no evidence on causal linkages?
Does this also mean that @Ball:1968ub is a "merely" descriptive paper according to the taxonomy of research papers outlined in Chapter \@ref(causal-inf).
How might the results of @Ball:1968ub be represented in a causal diagram assuming that accounting information is meaningful and markets are efficient?
Would an alternative causal diagram be assumed by a critic who viewed accounting information as meaningless?

4. Describe how Figure 1 of @Ball:1968ub supports each of principal results identified by @Ball:2019wu.

5. Consider the causal diagrams you created above.
Do the results of @Ball:1968ub provide more support for one causal diagram than the other.	

6. Compare Figure 1 of @Ball:2019wu with Figure 1 of BB68. 
What is common between the two figures? 
What is different?

7.	What does "less their average" mean in the title of Figure 1 of @Ball:2019wu? 
What effect does this have on the plot? 
(Does it make this plot different from Figure 1 of BB68? 
Is information lost in the process?)

8.	On p.418 of @Ball:2019wu, the authors say, "in this replication we address two issues with the BB68 significance tests."
Do you understand the points being made here? 

9. Also, on p.418, @Ball:2019wu say "the persistence of PEAD over time is evidence it does not constitute market inefficiency."
What do you make of this argument?

10. What is the minimum amount of information that our hypothetical genie needs to provide to enable formation of the portfolios underlying *TI*, *NI*, and *II*?
What are the rules for construction of each of these portfolios?

11. @Ball:1968ub observe a ratio of *NI* to *TI* of about 0.23. 
What do we expect this ratio to be?
Does this ratio depend on the information content of accounting information?

12.	Consider the paragraph in @Ball:2019wu [p. 418] beginning "an innovation in BB68 was to estimate ...". 
How do the discussions of these results differ between @Ball:1968ub and @Ball:2019wu?

13. Consider column (4) of Table 2 of @Ball:2019wu.
Is an equivalent set of numbers reported in BB68?
What is the underlying investment strategy associated with this column (this need not be feasible in practice)?

14. Heading 6.3 of @Ball:2019wu is "Does 'useful' disprove 'meaningless'?" 
Do you think that "not meaningless" implies "not useless"?
Which questions (or facts) does BB68 address in these terms?

## Replicating @Ball:1968ub

```{r wrds_data}
#| include: false
db <- dbConnect(duckdb::duckdb())

msf <- load_parquet(db, "msf", "crsp")
msi <- load_parquet(db, "msi", "crsp")
ccmxpf_lnkhist <- load_parquet(db, "ccmxpf_lnkhist", "crsp")
stocknames <- load_parquet(db, "stocknames", "crsp")

funda <- load_parquet(db, "funda", "comp")
fundq <- load_parquet(db, "fundq", "comp")

fundq_mod <-
  fundq |>
  filter(indfmt == "INDL", datafmt == "STD",
         consol == "C", popsrc == "D") |>
  filter(fqtr == 4, fyr == 12, !is.na(rdq)) |>
  select(gvkey, datadate, rdq) |>
  mutate(rdq_month = as.Date(floor_date(rdq, unit = "month"))) |>
  compute()

crsp_dates <-
  msi |>
  select(date) |>
  window_order(date) |>
  mutate(td = row_number()) |>
  mutate(month = as.Date(floor_date(date, unit = "month"))) |>
  compute()

td_link <-
  crsp_dates |>
  select(month, td) |> 
  rename(rdq_td = td, rdq_month = month) |>
  mutate(td = generate_series(rdq_td - 11L, rdq_td + 6L)) |>
  mutate(td = unnest(td)) |>
  inner_join(crsp_dates, by = "td") |>
  mutate(rel_td = td - rdq_td) |>
  select(rdq_month, rel_td, date) |>
  compute()

ccm_link <-
  ccmxpf_lnkhist |>
  filter(linktype %in% c("LC", "LU", "LS"),
         linkprim %in% c("C", "P")) |>
  rename(permno = lpermno) |>
  select(gvkey, permno, linkdt, linkenddt) |>
  compute()

rets_all <-
  fundq_mod |> 
  inner_join(td_link, by = "rdq_month") |>
  inner_join(ccm_link, by = "gvkey") |>
  filter(rdq_month >= linkdt, rdq_month <= linkenddt | is.na(linkenddt)) |>
  inner_join(msf, by = c("permno", "date")) |>
  inner_join(stocknames, by = "permno") |>
  filter(between(date, namedt, nameenddt),
         exchcd %in% c(1, 2, 3)) |>
  select(gvkey, datadate, rel_td, permno, date, ret) |>
  filter(between(year(datadate), 1987L, 2002L)) |>
  compute()

full_panel <-
  rets_all |> 
  group_by(gvkey, datadate) |> 
  mutate(n_obs = n()) |> 
  ungroup() |> 
  filter(n_obs == max(n_obs)) |>
  select(gvkey, datadate) |>
  compute()

rets <-
  rets_all |>
  semi_join(full_panel, by = c("gvkey", "datadate")) 

me_values <- 
  msf |> 
  mutate(mktcap = abs(prc) * shrout/1000.0) |> 
  select(permno, date, mktcap) |>
  mutate(month = as.Date(floor_date(date, unit = "month"))) |>
  filter(month(month) == 12) |>
  compute()

funda_mod <-
  funda |>
  filter(indfmt == "INDL", datafmt == "STD",
         consol == "C", popsrc == "D") 

news <-
  funda_mod |>
  filter(fyr == 12) |>
  group_by(gvkey) |>
  window_order(datadate) |>
  mutate(lag_ibc = lag(ibc),
         lag_oancf = lag(oancf),
         lag_at = lag(at),
         lag_fyear = lag(fyear)) |>
  filter(between(fyear, 1987, 2002),
         lag_fyear + 1 == fyear) |>
  mutate(earn_chg = if_else(lag_at > 0, (ibc - lag_ibc)/lag_at, NA_real_),
         cfo_chg = if_else(lag_at > 0, (oancf - lag_oancf)/lag_at, NA_real_),
         earn_gn = earn_chg > 0,
         cfo_gn = cfo_chg > 0) |>
  filter(!is.na(cfo_gn), !is.na(earn_gn)) |>
  ungroup() |>
  select(gvkey, datadate, earn_chg, cfo_chg, earn_gn, cfo_gn) |>
  group_by(datadate) |> 
  mutate(earn_decile = ntile(earn_chg, 10),
         cfo_decile = ntile(cfo_chg, 10)) |>
  ungroup() |>
  compute()
```

```{r size_rets_raw}
#| include: false

# Download the data
t <- tempfile(fileext = ".zip")
url <- paste0("http://mba.tuck.dartmouth.edu",
              "/pages/faculty/ken.french/ftp/",
              "Portfolios_Formed_on_ME_CSV.zip")
download.file(url, t)

# Determine breakpoints (lines) for different tables
temp <- read_lines(t)
vw_start <- grep("^\\s+Value Weight Returns -- Monthly", temp)
vw_end <- grep("^\\s+Equal Weight Returns -- Monthly", temp) - 4

ew_start <- grep("^\\s+Equal Weight Returns -- Monthly", temp)
ew_end <- grep("^\\s+Value Weight Returns -- Annual", temp) - 4

read_data <- function(start, end) {

  Sys.setenv(VROOM_CONNECTION_SIZE = 500000)
  
  fix_names <- function(names) {
    gsub("^$", "date", names)
  }

  read_csv(t, skip = start, n_max = end - start,
           na = c("-99.99"),
           name_repair = fix_names,
           show_col_types = FALSE) |>
    mutate(month = ymd(paste0(date, "01"))) |>
    select(-date) |>
    pivot_longer(names_to = "quantile",
                 values_to = "ret",
                 cols = -month) |>
    mutate(ret = ret / 100,
           decile = case_when(quantile == "Hi 10" ~ "10",
                              quantile == "Lo 10" ~ "1",
                              grepl("^Dec ", quantile) ~
                                sub("^Dec ", "", quantile),
                              TRUE ~ NA),
           decile = as.integer(decile)) |>
    filter(!is.na(decile)) |>
    select(-quantile)
}

vw_rets <- 
  read_data(vw_start, vw_end) |>
  rename(vw_ret = ret)

ew_rets <- 
  read_data(ew_start, ew_end) |>
  rename(ew_ret = ret)

size_rets <-
  ew_rets |>
  inner_join(vw_rets, by = c("month", "decile")) |>
  select(month, decile, everything()) |>
  copy_to(db, df = _, name = "size_rets")
```

```{r me_breakpoints}
#| include: false
t <- tempfile(fileext = ".zip")
url <- paste0("http://mba.tuck.dartmouth.edu",
              "/pages/faculty/ken.french/ftp/",
              "ME_Breakpoints_CSV.zip")
download.file(url, t)

temp <- read_lines(t)

me_breakpoints_raw <- 
  read_csv(t, skip = 1, 
           col_names = c("month", "n",
                         paste0("p", seq(from = 5, to = 100, by = 5))),
           col_types = "c",
           n_max = grep("^Copyright", temp) - 3) |>
  mutate(month = ymd(paste0(month, "01"))) |>
  select(-ends_with("5"), -n) |>
  pivot_longer(cols = - month,
               names_to = "decile",
               values_to = "cutoff") |>
  mutate(decile = gsub("^p(.*)0$", "\\1", decile)) |>
  mutate(decile = as.integer(decile)) 

me_breakpoints <-
  me_breakpoints_raw |>
  group_by(month) |> 
  arrange(decile) |> 
  mutate(me_min = coalesce(lag(cutoff), 0), me_max = cutoff) |> 
  mutate(me_max = if_else(decile == 10, Inf, me_max)) |>
  select(-cutoff) |>
  arrange(month, decile) |>
  copy_to(db, df = _, name = "me_breakpoints")
```

```{r}
#| include: false
me_decile_assignments <-
  me_breakpoints |>
  inner_join(me_values, by = "month") |>
  ungroup() |>
  mutate(dec_match = mktcap >= me_min & mktcap < me_max) |>
  filter(dec_match) |>
  mutate(year = as.integer(year(date)) + 1L) |>
  select(permno, year, decile) 
```

```{r}
#| include: false
merged <- 
  news |>
  mutate(year = year(datadate)) |>
  inner_join(rets, by = c("gvkey", "datadate")) |>
  inner_join(me_decile_assignments, by = c("permno", "year")) |>
  mutate(month = floor_date(date, unit = "month")) |>
  inner_join(size_rets, by = c("decile", "month")) |>
  select(-permno, -month) |>
  compute()
```

```{r}
#| include: false
plot_data <-
  merged |> 
  filter(!is.na(ret)) |>
  group_by(gvkey, datadate) |>
  window_order(rel_td) |>
  mutate(across(ends_with("ret"), ~ exp(cumsum(log(1 + .x))))) |>
  group_by(rel_td, earn_gn) |> 
  summarize(across(ends_with("ret"), ~ mean(.x, na.rm = TRUE)),
            .groups = "drop") |>
  mutate(aret_ew = ret - ew_ret, aret_vw = ret - ew_ret) |>
  compute()
```

```{r}
#| echo: false
plot_data |>
  ggplot(aes(x = rel_td, y = aret_ew, line = earn_gn)) +
  geom_line()
```

### Exercises

```{r}
#| include: false
max_cutoff <-
  me_breakpoints_raw |> 
  filter(month == "2020-12-01", decile == 10) |>
  select(cutoff) |>
  pull()
```

1. From the data below, we see that the upper bound for the tenth decile is about `r paste0("US$", prettyNum(max_cutoff/1000, digits = 3))` billion.
How can we reconcile this with the existence of firms with market capitalizations over US$1 trillion?
*Bonus*: Using data from `crsp.msf`, identify the firm whose market capitalization was `r paste0("US$", prettyNum(max_cutoff/1000, digits = 3))` billion in December 2020?
(*Hint*: For the bonus question, you can add a filter to the following code to obtain the answer.
Why do we need to group by `permco` instead of `permno` to find the answer?)

```{r}
me_breakpoints_raw |> 
  filter(month == "2020-12-01")
```

```{r}
#| include: false
#| eval: false
# To use this code, remove "eval: false" above. To include this code in your submission, remove "include: false" above.
# We create a new connection to Postgres here so that you don't # need to run the time-consuming code above to answer this
# question.

msf <- load_parquet(db, "msf", "crsp")

msf |> 
  filter(date == "2020-12-31") |>
  mutate(mktcap = abs(prc) * shrout/1000.0) |>
  group_by(permco, date) |>
  mutate(totmktcap = sum(mktcap, na.rm = TRUE)) |>
  select(permno, permco, date, mktcap, totmktcap) |>
  arrange(totmktcap)
```

2. To keep things straightforward, we focused on firms who have returns for each month in the $(t - 11, t + 6)$ window.
Can you tell what approach @Nichols:2004tb took with regard to this issue?

3. Table 2 of @Nichols:2004tb measures cumulative abnormal returns as the "cumulative raw return minus cumulative size decile portfolio to which the firm begins."
Apart from the use of a size-decile portfolio rather than some other market index, how does this measure differ from the Abnormal Performance Index (API) defined on p.168 of @Ball:1968ub?
Adjust the measure depicted in the replication of Figure 1 to more closely reflect the API definition used in @Ball:1968ub (but retaining the size-decile as the benchmark).
Does this tweak significantly affect the results?
Which approach seems most appropriate?
That of @Nichols:2004tb or that of @Ball:1968ub?
4. Create an alternative version of the figure above using the sign of "news" about cash flows in the place of income news.
Do your results broadly line up with those in Panel A of Figure 2 of @Nichols:2004tb?
Do these results imply that accounting income is inherently more informative than cash flows from operations?
Why or why not?

5. Create an alternative version of the figure above focused on the extreme earnings deciles in place of the good-bad news dichotomy.
Do your results broadly line up with those in Panel B of Figure 2 of @Nichols:2004tb?

6. Calculate *AI* by year following the formula on p. 175 of @Ball:1968ub (there denoted as $II_0$).
You may find it helpful to start with the code producing `plot_data` above.
You may also find it helpful to use the function `pivot_wider` to get information about portfolios in each year into a single row.
Note that you will only be interested in rows at $t = 0$ (e.g., `filter(rel_td == 0)`).

7. Calculate *NI* by year following the formula on p. 175 of @Ball:1968ub (there denoted as $NI_0$).
Note that you will only be interested in rows at $t = 0$ (e.g., `filter(rel_td == 0)`).
  
8. Using the data on *NI* and *AI* from above, create a plot of $AI/NI$ like that in Figure 2 of @Ball:2019wu.
Do you observe similar results to those shown in Figure 2 of @Ball:2019wu?

```{r}
#| include: false
dbDisconnect(db, shutdown = TRUE)
```

## References {-}